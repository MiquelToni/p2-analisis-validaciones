{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U1 Actividad NLP\n",
    "\n",
    "En esta actividad se ha analizado el conjunto de datos de [OpinRankDatasetWithJudgments](http://kavita-ganesan.com/entity-ranking-data/) para hallar las respuestas a las preguntas que propone el enunciado.\n",
    "\n",
    "Primero, nos descargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't downloaded the dataset, then uncomment and run. Note unzipping is os dependent\n",
    "# !curl https://github.com/kavgan/OpinRank/raw/master/OpinRankDatasetWithJudgments.zip -o \"data/OpinRankDatasetWithJudgments.zip\"\n",
    "# !7z x -y \"data/OpinRankDatasetWithJudgments.zip\" -odata # 7zip / windows\n",
    "# !unzip \"data/OpinRankDatasetWithJudgments.zip\" # mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos\n",
    "\n",
    "Dentro del zip que hemos descargados encontramos la siguiente estructura de ficheros:\n",
    "\n",
    "- data (dir)\n",
    "  - cars (dir)\n",
    "  - hotels (dir)\n",
    "    - data (dir)\n",
    "      - city-name (dir)\n",
    "        - hotel-name (file)\n",
    "          - *reviews here*\n",
    "      - city-name.csv (file)\n",
    "        - *hotels information*\n",
    "    - judgments\n",
    "      - city-name\n",
    "\n",
    "Ignoramos los datos de coches, directorio `cars`, y nos centraremos en el directorio `hotels`. Dentro de `hotels` encontramos dos directorios, `data` y `judgments`. \n",
    "\n",
    "En el primer directorio, data, encontramos los datos sin procesar. Estos datos se agrupan por ciudades. Cada ciudad tiene un fichero `.csv` y un directorio con su nombre. En el csv encontramos la lista de hoteles de esa ciudad y datos asociados a estos. Entre estos datos destaca el atributo `doc_id`, que contiene el nombre del fichero donde se encuentran las review de los usuarios. El fichero de las review se encuentra dentro del directorio con el nombre de la ciudad a la que pertenece.\n",
    "\n",
    "El `judgments` encontramos un directorio por cada ciudad. Dentro de cada directorio encontramos los resultados de las puntuaciones de relevancia.\n",
    "\n",
    "Para información más detallada podéis consultar la documentación del dataset, `OpinRankDatasetWithJudgments.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hotels = pd.DataFrame()\n",
    "\n",
    "# Podemos iterar y unir todos los csv porque sabemos que solo hay 10 ciudades.\n",
    "# Alternativamente, usariamos una función para cargar cada ciudad de forma independiente \n",
    "# como se ha hecho con las review de cada hotel.\n",
    "for name in os.listdir(\"data/hotels/data\"):\n",
    "    if name.endswith(\".csv\"):\n",
    "      hotelTmp = pd.read_csv(f\"data/hotels/data/{name}\", delimiter=',', index_col=False)\n",
    "      hotels = pd.concat([hotels, hotelTmp], axis=0)\n",
    "\n",
    "# Guardamos todos los nombres de las ciudades\n",
    "cities = hotels.city.unique()\n",
    "\n",
    "hotels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hay muchas reviews vamos a cargar el listado de reviews de cada hotel a demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# from textblob import TextBlob\n",
    "# def spellcheck(inputText):\n",
    "#   blob = TextBlob(str(inputText))\n",
    "#   return str(blob.correct())\n",
    "\n",
    "# Dada una fila/row del dataframe hotels del apartado anterior, nos devuelve un dataframe con las reviews\n",
    "def getReviews(hotel):\n",
    "  docId = hotel[\"doc_id\"]\n",
    "  city = hotel[\"city\"].replace(\" \", \"-\")\n",
    "  fileName = f\"data/hotels/data/{city}/{docId}\"\n",
    "  try:\n",
    "    reviews = pd.read_csv(fileName, delimiter=\"\\t\", index_col=False, header=None, encoding=\"cp1252\", on_bad_lines='skip')\n",
    "    reviews.columns = [\"date\", \"title\", \"review\", \"empty\"]\n",
    "\n",
    "    # la columna de date no la vamos a usar y la ultima columna, empty, es resultado de tener un tabulador\n",
    "    # a final de cada linea. No hay datos. Por lo tanto las quitamos\n",
    "    reviews = reviews.drop(columns=[\"date\", \"empty\"]) \n",
    "\n",
    "    # corregir ortografía. Es muuuuy lento\n",
    "    # reviews[\"title\"] = reviews[\"title\"].apply(spellcheck)\n",
    "    # reviews[\"review\"] = reviews[\"review\"].apply(spellcheck)\n",
    "\n",
    "    return reviews\n",
    "  except FileNotFoundError:\n",
    "    return pd.DataFrame()\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --targets 0\n",
    "# test the function\n",
    "reviews = getReviews(hotels.iloc[0])\n",
    "print(f\"cantidad de reviews: {len(reviews)}\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de carga de las reviews vimos que los ficheros que contienen las review:\n",
    "- Tienen una review por fila.\n",
    "- El separador es un tabulador.\n",
    "- El encode del fichero es `cp1252`. Se puede abrir en `utf-8` y es legible excepto acentos y algunos caracteres especiales.\n",
    "\n",
    "En el ejemplo podemos comprobar que la columna `date` no siempre va a tener valor.\n",
    "\n",
    "Actualización:\n",
    "  - Se ha modificado el algoritmo para que incluya soporte para corregir typos y se ha quitado la columna `date` ya que no aporta valor.\n",
    "  - Se ha comentado el corrector. Se tarda en mi maquina unos 10minutos para obtener 250 reviews. Sin el corrector son 0.6s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué partes de una habitación son las más mencionadas?\n",
    "\n",
    "In this section, I'm going to use wordnet of each room to look for matches. First, I'm going to look for the definition of room that we need. Then use it's hyponyms to search in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# load dependencies\n",
    "import re\n",
    "import tqdm\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "# nltk.download(\"punkt\")\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pprint\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "# check the room definition that we need\n",
    "for sense in wordnet.synsets('room'):\n",
    "    print(sense)\n",
    "    print(sense.definition())\n",
    "    print(sense.examples())\n",
    "    print(\"-\"*10)\n",
    "\n",
    "# Synset('room.n.01') is the definition that I'm looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "room = wordnet.synset('room.n.01')\n",
    "room.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones nos servirán para lanzar y recuperar los procesos en paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# reduce function\n",
    "def concatCounter(dict1, dict2):\n",
    "  for key in dict1.keys():\n",
    "    if key in dict2:\n",
    "      dict2[key] += dict1[key]\n",
    "    else:\n",
    "      dict2[key] = dict1[key]\n",
    "  return dict2\n",
    "\n",
    "def parallelize_dataframe(df, func, reduceFunc):\n",
    "  df_split = np.array_split(df, os.cpu_count())\n",
    "  # get a view on the cluster\n",
    "  view = rc.load_balanced_view()\n",
    "  # submit the tasks\n",
    "  asyncresult = view.map_async(func, df_split)\n",
    "  # wait interactively for results\n",
    "  asyncresult.wait_interactive()\n",
    "  # retrieve actual results\n",
    "  result = functools.reduce(reduceFunc,asyncresult.get())\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones a continuación serán usadas en cada thread / cluster.\n",
    "En el primer bloque hay las genéricas.\n",
    "En el segundo bloque las funciones de búsqueda de habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "def addOne(dict, key):\n",
    "  if key in dict:\n",
    "    dict[key] += 1\n",
    "  else:\n",
    "    dict[key] = 1\n",
    "  return dict\n",
    "\n",
    "def getNouns(review):\n",
    "  nouns = []\n",
    "  cleanText = wordCleaner(review)\n",
    "  tokens = list(set(word_tokenize(cleanText)))\n",
    "  taggedTokens = pos_tag(tokens, tagset='universal')\n",
    "  for taggedToken in taggedTokens:\n",
    "    (word, tokenType) = taggedToken\n",
    "    if tokenType == \"NOUN\":\n",
    "      nouns.append(word)\n",
    "  return nouns\n",
    "\n",
    "def wordCleaner(text):\n",
    "  return re.sub(\"(\\W(?!(\\w)))+\", \" \", str(text)).lower()\n",
    "\n",
    "def getSynsetsNames(synsets):\n",
    "  return list(map(lambda synset: synset.name(), synsets))\n",
    "\n",
    "rooms = getSynsetsNames(wordnet.synset('room.n.01').hyponyms()) # get room names as \"constant\", so we don't recalculate every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def processReview(counter, review):\n",
    "  text = str(review.title) + \" \" + str(review.review)\n",
    "  tokenizedText = getNouns(text)\n",
    "  for word in tokenizedText:\n",
    "    try:\n",
    "      roomType = getRoomType(word)\n",
    "      if roomType != None:\n",
    "        counter = addOne(counter, roomType)\n",
    "    except Exception as e:\n",
    "      print(f\"failed with word '{word}' with type '{roomType}' and error code: {e}\")\n",
    "  return counter\n",
    "\n",
    "def getRoomType(word):\n",
    "  wordSynsets = wordnet.synsets(str(word))\n",
    "  for wordSynset in getSynsetsNames(wordSynsets):\n",
    "    if wordSynset in rooms:\n",
    "      return wordSynset\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `findAllRoomMentions` al ser la principal la definimos en local porque debe ser visible para la función `map` (dentro de `parallelize_dataframe`) y enviarla al cluster a ejecutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllRoomMentions(hotels):\n",
    "  counter = {}\n",
    "  for index, hotel in hotels.iterrows():\n",
    "    reviews = getReviews(hotel)\n",
    "    for reviewIdx, review in reviews.iterrows():\n",
    "      counter = processReview(counter, review)\n",
    "  return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos 12 hoteles para probar que los resultados son los que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "parallelize_dataframe(hotels[0:12], findAllRoomMentions, concatCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el proceso para todos los hoteles agrupados por ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "for city in cities:\n",
    "  stats[city] =  parallelize_dataframe(hotels[hotels[\"city\"] == city], findAllRoomMentions, concatCounter)\n",
    "\n",
    "statsDf = pd.DataFrame(stats).replace('NaN', 0)\n",
    "statsDf.to_csv('resultados_apartado_1.csv')\n",
    "statsDf.plot.barh(figsize=(16,10), stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ¿Qué servicios pueden detectarse por cada hotel?\n",
    "Ejecutamos el mismo código de búsqueda anterior mortificándolo para que esta vez busque servicios.\n",
    "\n",
    "Antes de que pueda buscar servicios debemos definir cuales son los *synset* de los servicios de un hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "# buscar las definiciones de \"que es un servicio\"\n",
    "hotelServices = [\"restaurant\", \"pool\", \"spa\", \"gym\", \"bellman\", \"wifi\", \"television\", \"excursion\", \"clean\", \"hotspot\"]\n",
    "for service in hotelServices:\n",
    "  for sense in wordnet.synsets(service):\n",
    "      print(sense)\n",
    "      print(sense.definition())\n",
    "      print(sense.examples())\n",
    "      print(sense.hyponyms())\n",
    "      print(sense.hypernyms())\n",
    "      print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los servicios que hemos seleccionado son los siguientes: // TODO: explain why\n",
    "- service.n.15\n",
    "- restaurant.n.01\n",
    "- pool.n.01\n",
    "- watering_place.n.01 # kinda spa\n",
    "- athletic_facility.n.01\n",
    "- baggageman.n.01\n",
    "- bellboy.n.01\n",
    "- lifeguard.n.01\n",
    "- checker.n.01\n",
    "- houseclean.v.01\n",
    "- wireless_local_area_network.n.01\n",
    "- television.n.01\n",
    "- excursion.n.01\n",
    "- gambling_house.n.01 \n",
    "- health_spa.n.01 \n",
    "- hotel-casino.n.01 \n",
    "- massage_parlor.n.02 \n",
    "- mercantile_establishment.n.01\n",
    "- spot.n.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "services = [\n",
    "  wordnet.synset('service.n.15'), # el servicio que da un camarero a los clientes\n",
    "  wordnet.synset('restaurant.n.01'),\n",
    "  wordnet.synset('pool.n.01'),\n",
    "  wordnet.synset('watering_place.n.01'), # kinda spa\n",
    "  wordnet.synset('athletic_facility.n.01'),\n",
    "  wordnet.synset('baggageman.n.01'),\n",
    "  wordnet.synset('bellboy.n.01'),\n",
    "  wordnet.synset('lifeguard.n.01'),\n",
    "  wordnet.synset('checker.n.01'),\n",
    "  wordnet.synset('houseclean.v.01'),\n",
    "  wordnet.synset('wireless_local_area_network.n.01'),\n",
    "  wordnet.synset('television.n.01'),\n",
    "  wordnet.synset('excursion.n.01'),\n",
    "  wordnet.synset('gambling_house.n.01'), \n",
    "  wordnet.synset('health_spa.n.01'), \n",
    "  wordnet.synset('hotel-casino.n.01'), \n",
    "  wordnet.synset('massage_parlor.n.02'), \n",
    "  wordnet.synset('mercantile_establishment.n.01'),\n",
    "  wordnet.synset('spot.n.07'),\n",
    "]\n",
    "serviceNames = getSynsetsNames(services)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función `getAllHypernyms` capaz de calcular todos los *hypernyms* dada una palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from collections import OrderedDict\n",
    "\n",
    "def removeDuplicates(list):\n",
    "  return OrderedDict.fromkeys(list).keys()\n",
    "\n",
    "def getAllHypernyms(itemName):\n",
    "  allItemHypernyms = []\n",
    "  for synset in wordnet.synsets(itemName):\n",
    "    allItemHypernyms = allItemHypernyms + getAllHypernymsOsSynset(synset)\n",
    "  return removeDuplicates(allItemHypernyms)\n",
    "\n",
    "def getAllHypernymsOsSynset(synset):\n",
    "  synsetHypernyms = synset.hypernyms()\n",
    "  if len(synsetHypernyms) == 0:\n",
    "    return []\n",
    "\n",
    "  allSynsetHypernyms = []\n",
    "  for hypernym in synsetHypernyms:\n",
    "    allSynsetHypernyms = allSynsetHypernyms + getAllHypernymsOsSynset(hypernym)\n",
    "\n",
    "  return synsetHypernyms + allSynsetHypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "# test\n",
    "pprint.pprint(getAllHypernyms('restaurant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedimiento de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def lookForServices(counter, review):\n",
    "  text = str(review.title) + \" \" + str(review.review)\n",
    "  tokenizedText = getNouns(text)\n",
    "  for word in tokenizedText:\n",
    "    try:\n",
    "      serviceType = getServiceType(word)\n",
    "      if serviceType != None:\n",
    "        counter = addOne(counter, word)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  return counter\n",
    "\n",
    "# En este caso en lugar de mirar si los synset de 1 nivel encajan con la lista de habitaciones posibles, vamos a hacerlo al revés.\n",
    "# Vamos a coger todos los hypernyms de la palabra y ver si encaja \n",
    "def getServiceType(word):\n",
    "  wordSynsets = getAllHypernyms(word)\n",
    "  for wordSynset in getSynsetsNames(wordSynsets):\n",
    "    if wordSynset in serviceNames:\n",
    "      return wordSynset\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllServices(hotels):\n",
    "  counterList = {}\n",
    "  for index, hotel in tqdm.tqdm(hotels.iterrows()):\n",
    "    reviews = getReviews(hotel)\n",
    "    counter = {}\n",
    "    for reviewIdx, review in reviews.iterrows():\n",
    "      lookForServices(counter, review)\n",
    "    counterList[hotel[\"hotel_name\"]] = counter\n",
    "  return counterList\n",
    "\n",
    "# test\n",
    "result = parallelize_dataframe(hotels[0:12], findAllServices, concatCounter)\n",
    "pd.DataFrame(result).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parallelize_dataframe(hotels, findAllServices, concatCounter)\n",
    "result = {k: v for k, v in result.items() if v} # remove empty results\n",
    "result2 = pd.DataFrame(result).transpose()\n",
    "result2.to_csv('resultados_apartado_2.csv')\n",
    "result2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Los servicios encontrados en las review de cada hotel no tiene porque ser del propio hotel.\n",
    "\n",
    "Comentar también que el synset de 'pool.n.01' puesto para detectar si el hotel tiene o no piscina, ha terminado detectando si hay desagües (sink/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ¿Qué lugares y qué porcentaje de revisiones sobre cada ciudad mencionan otras ciudades, regiones o puntos de interés turístico?\n",
    "\n",
    "En este apartado en lugar de usar nltk vamos a usar spacy. Spacy, durante la tokenización nos etiqueta el tipo de palabra que es. En nuestro caso buscaremos por las palabras que se etiqueten como `GPE`, \"Geopolitical entity, i.e. countries, cities, states\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGpeMentions(hotels):\n",
    "  gpeMentions = {}\n",
    "  for index, hotel in hotels.iterrows():\n",
    "    reviews = getReviews(hotel)\n",
    "    for reviewIdx, review in reviews.iterrows():\n",
    "      text = str(review.title) + \". \" + str(review.review)\n",
    "      cleanText = wordCleaner(text)\n",
    "      tokens = nlp(cleanText)\n",
    "      for ent in tokens.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "          gpeMentions = addOne(gpeMentions, ent.lower_)\n",
    "      gpeMentions = addOne(gpeMentions, \"reviewsAnalyzed\")\n",
    "  return gpeMentions\n",
    "\n",
    "#test\n",
    "test_result = parallelize_dataframe(hotels[10:12], findGpeMentions, concatCounter)\n",
    "pprint.pprint(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGpeMentions = {}\n",
    "\n",
    "for city in cities:\n",
    "  allGpeMentions[city] = parallelize_dataframe(hotels[hotels[\"city\"] == city], findGpeMentions, concatCounter)\n",
    "\n",
    "allGpeMentionsDf = pd.DataFrame.from_dict(allGpeMentions).fillna(0)\n",
    "allGpeMentionsDf.to_csv(\"resultados_apartado_3.csv\")\n",
    "allGpeMentionsDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por ejemplo, con vuestra información podríamos llegar a responder a la pregunta: ¿en Las Vegas se menciona el Gran Cañón del Colorado?\n",
    "Si, para ello tendríamos que buscar en el resultado de búsqueda `allGpeMentions` por la key, `Las Vegas` y dentro de las menciones de Las Vegas buscar si existe la key `Gran Cañón del Colorado`, en el idoma de las reviews, inglés.\n",
    "Hay que tener en cuenta que no se corregido el input del usuario, por lo que resultados como \"Gran canyon\" o \"Colorado canyon\" aparecerían como sitios distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasVegasGpe = allGpeMentions.get(\"las vegas\", {})\n",
    "granCanyonGpe = lasVegasGpe.get(\"gran canyon\", \"No se ha encontrado 'gran canyon'\")\n",
    "pprint.pprint(lasVegasGpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscando con regex a mano si he encontrado \"gran canyon\". A continuación el porqué no se detecta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# review 320 de usa_nevada_las-vegas_excalibur_hotel_casino\n",
    "text = \"fantastic value for money at the excalibur\tMy husband and i stayed at the excalibur from the 6th to 16th april we had a wonderful time, this hotel is in a great spot on strip, we were in tower 2 and the room was spotless, cleaned every day, food was great value from buffet bar, staff were very friendly and helpful, if you are going to visit gran canyon book with sundance they are a class act just a fantastic place to visit las vegas cant wait to go back\"\n",
    "\n",
    "train = nlp(text)\n",
    "displacy.render(train, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gran canyon se etiqueta como nombre de persona en lugar de una ubicación, `GPE`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
