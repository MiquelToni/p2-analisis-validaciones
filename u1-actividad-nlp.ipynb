{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U1 Actividad NLP\n",
    "\n",
    "En esta actividad se ha analizado el conjunto de datos de [OpinRankDatasetWithJudgments](http://kavita-ganesan.com/entity-ranking-data/) para hallar las respuestas a las preguntas que propone el enunciado.\n",
    "\n",
    "Primero, nos descargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't downloaded the dataset, then uncomment and run. Note unzipping is os dependent\n",
    "# !curl https://github.com/kavgan/OpinRank/raw/master/OpinRankDatasetWithJudgments.zip -o \"data/OpinRankDatasetWithJudgments.zip\"\n",
    "# !7z x -y \"data/OpinRankDatasetWithJudgments.zip\" -odata # 7zip / windows\n",
    "# !unzip \"data/OpinRankDatasetWithJudgments.zip\" # mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos\n",
    "\n",
    "Dentro del zip que hemos descargados encontramos la siguiente estructura de ficheros:\n",
    "\n",
    "- data (dir)\n",
    "  - cars (dir)\n",
    "  - hotels (dir)\n",
    "    - data (dir)\n",
    "      - city-name (dir)\n",
    "        - hotel-name (file)\n",
    "          - *reviews here*\n",
    "      - city-name.csv (file)\n",
    "        - *hotels information*\n",
    "    - judgments\n",
    "      - city-name\n",
    "\n",
    "Ignoramos los datos de coches, directorio `cars`, y nos centraremos en el directorio `hotels`. Dentro de `hotels` encontramos dos directorios, `data` y `judgments`. \n",
    "\n",
    "En el primer directorio, data, encontramos los datos sin procesar. Estos datos se agrupan por ciudades. Cada ciudad tiene un fichero `.csv` y un directorio con su nombre. En el csv encontramos la lista de hoteles de esa ciudad y datos asociados a estos. Entre estos datos destaca el atributo `doc_id`, que contiene el nombre del fichero donde se encuentran las review de los usuarios. El fichero de las review se encuentra dentro del directorio con el nombre de la ciudad a la que pertenece.\n",
    "\n",
    "El `judgments` encontramos un directorio por cada ciudad. Dentro de cada directorio encontramos los resultados de las puntuaciones de relevancia.\n",
    "\n",
    "Para información más detallada podéis consultar la documentación del dataset, `OpinRankDatasetWithJudgments.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "hotels = pd.DataFrame()\n",
    "\n",
    "# Podemos iterar y unir todos los csv porque sabemos que solo hay 10 ciudades.\n",
    "# Alternativamente, usariamos una función para cargar cada ciudad de forma independiente \n",
    "# como se ha hecho con las review de cada hotel.\n",
    "for name in os.listdir(\"data/hotels/data\"):\n",
    "    if name.endswith(\".csv\"):\n",
    "      hotelTmp = pd.read_csv(f\"data/hotels/data/{name}\", delimiter=',', index_col=False)\n",
    "      hotels = pd.concat([hotels, hotelTmp], axis=0)\n",
    "\n",
    "# Guardamos todos los nombres de las ciudades\n",
    "cities = hotels.city.unique()\n",
    "\n",
    "hotels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dada una fila/row del dataframe hotels del apartado anterior, nos devuelve un dataframe con las reviews\n",
    "def getReviews(hotel):\n",
    "  docId = hotel[\"doc_id\"]\n",
    "  city = hotel[\"city\"]\n",
    "  fileName = f\"data/hotels/data/{city}/{docId}\"\n",
    "  try:\n",
    "    reviews = pd.read_csv(fileName, delimiter=\"\\t\", index_col=False, header=None, encoding=\"iso-8859-1\")\n",
    "    reviews.columns = [\"date\", \"title\", \"review\", \"empty\"]\n",
    "    reviews = reviews.drop(columns=\"empty\") # last column is generated by a tab before end of line\n",
    "    return reviews\n",
    "  except FileNotFoundError:\n",
    "    return pd.DataFrame()\n",
    "  except Exception as e:\n",
    "    if hasattr(e, 'message'):\n",
    "      print(f\"No se ha podido leer las reviews de {fileName}. Error {e.message}\")\n",
    "    else:\n",
    "      print(e)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# test the function\n",
    "reviews = getReviews(hotels.iloc[10])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el proceso de carga de las reviews vimos que los ficheros que contienen las review:\n",
    "- Tienen una review por fila.\n",
    "- El separador es un tabulador.\n",
    "- El encode del fichero es `iso-8859-1`. Se puede abrir en `utf-8` y es legible excepto acentos y algunos caracteres especiales.\n",
    "\n",
    "En el ejemplo podemos comprobar que la columna `date` no siempre va a tener valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué partes de una habitación son las más mencionadas?\n",
    "\n",
    "In this section, I'm going to use wordnet of each room to look for matches. First, I'm going to look for the definition of room that we need. Then use it's hyponyms to search in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import re\n",
    "import tqdm\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')\n",
    "# nltk.download(\"punkt\")\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the room definition that we need\n",
    "for sense in wordnet.synsets('room'):\n",
    "    print(sense)\n",
    "    print(sense.definition())\n",
    "    print(sense.examples())\n",
    "    print(\"-\"*10)\n",
    "\n",
    "# Synset('room.n.01') is the definition that I'm looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room = wordnet.synset('room.n.01')\n",
    "room.hyponyms()\n",
    "\n",
    "# now we need to \"correct\" the spelling of user words. Ex: some reviews wrote \"bathrom\" instate of \"bathroom\". \n",
    "\n",
    "\n",
    "# https://subscription.packtpub.com/book/application-development/9781782167853/1/ch01lvl1sec16/calculating-wordnet-synset-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define auxiliary functions\n",
    "\n",
    "def getNouns(review):\n",
    "  nouns = []\n",
    "  cleanText = wordCleaner(review)\n",
    "  tokens = word_tokenize(cleanText)\n",
    "  taggedTokens = pos_tag(tokens, tagset='universal')\n",
    "  for taggedToken in taggedTokens:\n",
    "    (word, tokenType) = taggedToken\n",
    "    if tokenType == \"NOUN\":\n",
    "      nouns.append(word)\n",
    "  return nouns\n",
    "\n",
    "def wordCleaner(text):\n",
    "  return re.sub(\"(\\W(?!(\\w)))+\", \" \", str(text)).lower()\n",
    "\n",
    "def getSynsetsNames(synsets):\n",
    "  return list(map(lambda synset: synset.name(), synsets))\n",
    "\n",
    "rooms = getSynsetsNames(room.hyponyms()) # get room names as \"constant\", so we don't recalculate every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllRoomMentions(hotels):\n",
    "  counter = {}\n",
    "  for index, hotel in tqdm.tqdm(hotels.iterrows()):\n",
    "    reviews = getReviews(hotel)\n",
    "    for reviewIdx, review in reviews.iterrows():\n",
    "      counter = findRoomMentions(counter, review)\n",
    "  return counter\n",
    "\n",
    "def findRoomMentions(counter, review):\n",
    "  text = str(review.title) + \" \" + str(review.review)\n",
    "  tokenizedText = getNouns(text)\n",
    "  for word in tokenizedText:\n",
    "    try:\n",
    "      roomType = getRoomType(word)\n",
    "      if roomType != None:\n",
    "        if roomType in counter:\n",
    "          counter[roomType] += 1\n",
    "        else:\n",
    "          counter[roomType] = 1\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  return counter\n",
    "\n",
    "def getRoomType(word):\n",
    "  wordSynsets = wordnet.synsets(word)\n",
    "  for wordSynset in getSynsetsNames(wordSynsets):\n",
    "    if wordSynset in rooms:\n",
    "      return wordSynset\n",
    "  return None\n",
    "\n",
    "# test\n",
    "findAllRoomMentions(hotels[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "# for city in cities:\n",
    "#   stats[city] = findAllRoomMentions(hotels[hotels[\"city\"] == city])\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ¿Qué servicios pueden detectarse por cada hotel?\n",
    "Ejecutamos el mismo código de búsqueda anterior mortificándolo para que esta vez busque servicios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar las definiciones de \"que es un servicio\"\n",
    "hotelServices = [\"restaurant\", \"pool\", \"spa\", \"gym\", \"bellman\", \"wifi\", \"television\", \"excursion\", \"clean\"]\n",
    "for service in hotelServices:\n",
    "  for sense in wordnet.synsets(service):\n",
    "      print(sense)\n",
    "      print(sense.definition())\n",
    "      print(sense.examples())\n",
    "      print(sense.hyponyms())\n",
    "      print(sense.hypernyms())\n",
    "      print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = [\n",
    "  wordnet.synset('service.n.15'), # el servicio que da un camarero a los clientes\n",
    "  wordnet.synset('restaurant.n.01'),\n",
    "  wordnet.synset('pool.n.01'),\n",
    "  wordnet.synset('watering_place.n.01'), # kinda spa\n",
    "  wordnet.synset('health_spa.n.01'),\n",
    "  wordnet.synset('athletic_facility.n.01'),\n",
    "  wordnet.synset('baggageman.n.01'),\n",
    "  wordnet.synset('bellboy.n.01'),\n",
    "  wordnet.synset('lifeguard.n.01'),\n",
    "  wordnet.synset('checker.n.01'),\n",
    "  wordnet.synset('houseclean.v.01'),\n",
    "  wordnet.synset('wireless_local_area_network.n.01'),\n",
    "  wordnet.synset('television.n.01'),\n",
    "  wordnet.synset('excursion.n.01'),\n",
    "]\n",
    "serviceNames = getSynsetsNames(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def removeDuplicates(list):\n",
    "  return OrderedDict.fromkeys(list).keys()\n",
    "\n",
    "def getAllHypernyms(itemName):\n",
    "  allItemHypernyms = []\n",
    "  for synset in wordnet.synsets(itemName):\n",
    "    allItemHypernyms = allItemHypernyms + getAllHypernymsOsSynset(synset)\n",
    "  return removeDuplicates(allItemHypernyms)\n",
    "\n",
    "def getAllHypernymsOsSynset(synset):\n",
    "  synsetHypernyms = synset.hypernyms()\n",
    "  if len(synsetHypernyms) == 0:\n",
    "    return []\n",
    "\n",
    "  allSynsetHypernyms = []\n",
    "  for hypernym in synsetHypernyms:\n",
    "    allSynsetHypernyms = allSynsetHypernyms + getAllHypernymsOsSynset(hypernym)\n",
    "\n",
    "  return synsetHypernyms + allSynsetHypernyms\n",
    "\n",
    "# test\n",
    "print(getAllHypernyms('restaurant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def findAllServices(hotels):\n",
    "  counterList = {}\n",
    "  for index, hotel in tqdm.tqdm(hotels.iterrows()):\n",
    "    reviews = getReviews(hotel)\n",
    "    counter = {}\n",
    "    for reviewIdx, review in reviews.iterrows():\n",
    "      lookForServices(counter, review)\n",
    "    counterList[hotel[\"hotel_name\"]] = counter\n",
    "  return counterList\n",
    "\n",
    "def lookForServices(counter, review):\n",
    "  text = str(review.title) + \" \" + str(review.review)\n",
    "  tokenizedText = getNouns(text)\n",
    "  for word in tokenizedText:\n",
    "    try:\n",
    "      serviceType = getServiceType(word)\n",
    "      if serviceType != None:\n",
    "        if serviceType in counter:\n",
    "          counter[serviceType] += 1\n",
    "        else:\n",
    "          counter[serviceType] = 1\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "  return counter\n",
    "\n",
    "# En este caso en lugar de mirar si los synset de 1 nivel encajan con la lista de habitaciones posibles, vamos a hacerlo al revés.\n",
    "# Vamos a coger todos los hypernyms de la palabra y ver si encaja \n",
    "def getServiceType(word):\n",
    "  wordSynsets = getAllHypernyms(word)\n",
    "  for wordSynset in getSynsetsNames(wordSynsets):\n",
    "    if wordSynset in rooms:\n",
    "      return wordSynset\n",
    "  return None\n",
    "\n",
    "# test\n",
    "pprint.pprint(findAllServices(hotels[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(findAllServices(hotels))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a707b6ce8c685eb936424fcc3009d4b4b7a52543c4db09380a3fc49186ceb509"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
